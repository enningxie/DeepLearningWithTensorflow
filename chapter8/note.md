# 循环神经网络
介绍循环神经网络（recurrent neural network, RNN）以及循环神经网络中的一个重要的结构（长短期记忆网络（long short-term memory, LSTM））。

先说明一个最简单的循环神经网络的前向传播是如何工作的；  
再介绍循环神经网络中最重要的结构--长短期记忆网络的网络结构；  
介绍一些常用的循环神经网络的变种；  
最后针对语言模型、时序预测两个问题，设计和使用循环神经网络。  

## 循环神经网络简介
传统的机器学习算法非常依赖于人工提取的特征，使得基于机器学习的图像识别、语音识别以及自然语言处理等问题存在特征提取的瓶颈。而基于全连接循环神经网络的方法也存在参数太多、无法利用数据中时间序列信息等问题。随着更加有效的循环神经网络结构被不断提出，循环神经网络挖掘数据中的时序信息以及语义信息的深度表达能力被充分利用，并在语音识别、语言模型、机器翻译以及时序分析等方面实现了突破。  

循环神经网络的主要用途是处理和预测序列数据。在之前介绍的全连接神经网络或卷积神经网络模型中，网络结构都是从输入层到隐含层再到输入层，层与层之间是全连接或部分连接的。但每层之间的节点是无连接的。考虑这样一个问题，如果要预测句子的下一个单词是什么，一般需要用到当前单词以及前面的单词，因为句子中前后单词并不是独立的。比如，当前单词是“很”，前一个单词是“天空”，那么下一个单词很大概率是“蓝”。循环神经网络的来源就是为了刻画一个序列当前的输出与之前信息的关系。从网络结构上，循环神经网络会记忆之前的信息，并利用之前的信息影响后面节点的输出。也就是说，循环神经网络的隐藏层之间的节点是有连接的，隐藏层的输入不仅包括输入层的输出，还包括上一时刻隐藏层的输出。  

图1，展示了一个典型的循环神经网络。对于循环神经网络，一个非常重要的概念就是时刻。循环神经网络会对每一个时刻的输入结合当前模型的状态给出一个输出。从图1可以看到，循环神经网络的主体结构A的输入除了来自输入层x_t，还有一个循环的边来提供当前的状态，并输出一个值h_t。同时A的状态会从当前传递到下一步。因此，循环神经网络理论上可以被看作是同一神经网络结构被无限复制的结果。但出于优化的考虑，目前循环神经网络无法做到真正的无限循环，所以，现实中一般会将循环体展开，于是可以得到图2所示的结构。  

在图2中可以更加清楚的看到循环神经网络在每一个时刻会有一个输入x_t，然后根据循环神经网络当前的状态A_t提供一个输出h_t。而循环神经网络当前的状态A_t是根据上一时刻的状态A_t-1和当前的输入X_t共同决定的。从循环神经网络的结构特征可以很容易得出它最擅长解决的问题是与时间序列相关的。循环神经网络也是处理这类问题时最自然的神经网络结构。对于一个序列数据，可以将这个序列上不同时刻的数据依次传入循环神经网络的输入层，而输出可以是对序列中下一个时刻的预测，也可以是对当前时刻信息的处理结果（比如语音识别结果）。循环神经网络要求每一个时刻都有一个输入，但是不一定每个时刻都有输出。在过去的几年中，循环神经网络已经被广泛地应用在语音识别、语言模型、机器翻译以及时序分析等问题上，并取得了巨大的成功。

以机器翻译为例来介绍循环神经网络是如何解决实际问题的。循环神经网络中每一个时刻的输入为需要翻译的句子中的单词。如图3，需要翻译的句子为ABCD，那么循环神经网络第一段每个时刻的输入就分别是A、B、C、D然后用_作为待翻译句子的结束符。在第一段中，循环神经网络没有输出。从_开始，循环神经网络进入翻译阶段。该阶段中每一个时刻的输入是上一个时刻的输出，而最终得到的输出就是句子ABCD翻译的结果。图3中可以看到句子ABCD对应的翻译结果就是XYZ，而Q是代表翻译结束的结束符。

如之前所介绍，循环神经网络可以被看作是同一神经网络在时间序列上被复制多次的结果，这个被复制多次的结构被称之为循环体。如何设计循环体的网络结构是循环神经网络解决实际问题的关键。和卷积神经网络过滤器中参数是共享的类似，在循环神经网络中，循环体网络结构中的参数在不同时刻也是共享的。

图4展示了一个使用最简单的循环体结构的循环神经网络，在这个循环体中只使用了一个类似全连接层的神经网络结构。下面将通过图4中所展示的神经网络来介绍循环神经网络前向传播的完整流程。循环神经网络中的状态是通过一个向量来表示的，这个向量的维度也称为循环神经网络隐藏层的大小，假设为h。从图4中可以看出，循环体中的神经网络的输入有两部分，一部分为上一时刻的状态，另一部分为当前时刻的输入样本。对于时间序列数据来说（比如不同时刻商品的销量），每一时刻的输入样例可以是当前时刻的数值（比如销量值）；对于语言模型来说，输入样例可以是当前单词对应的单词向量。

假设输入向量的维度为x，那么图4中循环体的全连接层神经网络的输入大小为h+x。也就是将上一时刻的状态与当前时刻的输入拼接成一个大的向量作为循环体中神经网络的输入。因为当前该神经网络的输出为当前时刻的状态，于是输出层的节点也是h，循环体中的参数个数为(h+x)*h+h个。从图4中可以看到。循环体中的神经网络输出不但提供给了下一时刻作为状态，同时也会提供给当前时刻的输出。为了将当前时刻的状态转化为最终的输出，循环神经网络还需要另外一个全连接神经网络来完成这个过程。这和卷积神经网络中最后的全连接层的意义是一样的。类似的。不同时刻用于输出的全连接神经网络中的参数也是一致的。为了让你们对循环神经网络的前向传播有一个更加直观的认识，图5展示了一个循环神经网络前向传播的具体计算过程。

todo

循环神经网络唯一的区别在于因为它每个时刻都有一个输出，所以循环神经网络的总损失为所有时刻（或部分时刻）上的损失函数的总和。

理论上循环神经网络可以支持任意长度的序列，然而在实际应用中，如果序列过长会导致优化时出现梯度消散的问题，所以实际中一般会规定一个最大长度，当序列长度超过规定长度之后会对序列进行截断。

## 长短时记忆网络（LTSM）结构
循环神经网络工作的关键点就是使用历史的信息来帮助当前的决策。例如使用之前出现的单词来加强对当前文字的理解。循环神经网络可以更好地利用传统神经网络结构所不能建模的信息，但同时，这也带来了更大的技术挑战——长期依赖问题。

在有些问题中，模型仅仅需要短期内的信息来执行当前的任务。比如预测短语“大海的颜色是蓝色”中的最后一个单词“蓝色”时，模型并不需要记忆这个短语之前更长的上下文信息——因为这一句话已经包含了足够的信息来预测最后一个单词。在这样的场景中，相关的信息和待预测的词之间的间隔很小，循环神经网络可以比较容易地利用先前的信息。

但同样也会有一些上下文场景更加复杂的情况。比如当模型试着去预测段落“某地开设了大量的工厂，空气污染十分严重。。。这里的天空都是灰色的”的最后一个单词时，仅仅更具短期依赖就无法很好的解决这种问题。因为只根据最后一小段，最后一个词可以是“蓝色的”或者灰色的。但如果模型需要预测清楚具体是什么颜色，就需要考虑先前提到但离当前位置较远的上下文信息。因此，当前预测位置和相关信息之间的文本间隔就有可能变得很大。当这个间隔不断增大时，类似图4给出的简单循环神经网络有可能会丧失学习到距离如此远的信息的能力。或者在复杂语言场景中，有用信息的间隔有大有小、长短不一，循环神经网络的性能也会受到限制。

长短时间记忆网络的设计就是为了解决这个问题，而循环神经网络被成功应用的关键就是LSTM。在很多的任务上，采用LSTM结构的循环神经网络比标准的循环神经网络表现更好。

接下来介绍LSTM结构，它是一种特殊的循环体结构，如图6所示，与单一的tanh循环体结构不同，LSTM是一种拥有三个门结构的特殊网络结构。LSTM靠这些门的结构让信息有选择性地影响循环神经网络中每个时刻的状态。所谓门结构就是使用一个sigmoid神经网络和一个按位做乘法的操作，这两个操作合在一起就是一个门的结构。之所以该结构叫做门是因为使用sigmoid作为激活函数的全连接神经网络层会输出一个0到1之间的数值，描述当前输入有多少信息量可以通过这个结构。于是这个结构的功能就类似于一扇门，当门打开时（sigmoid神经网络层输出为1时），全部信息都可以通过；当门关上时（ssigmoid神经网络层输出为0时），任何信息都无法通过。

下面介绍每一个门是如何工作的。为了使循环神经网络更有效的保存长期网络，图6中“遗忘门”和“输入门”至关重要，它们是LSTM结构的核心。遗忘门的作用是让循环神经网络忘记之前没有用的信息。比如一段文章中先介绍了某地原来是绿水蓝天，但后来被污染了。于是在看到被污染了之后，循环神经网络应该忘记之前是绿水蓝天的状态。这个工作是通过遗忘门来完成的。遗忘门会根据当前的输入x_t，上一时刻状态c_t-1和上一时刻输出h_t-1共同决定哪一部分记忆需要被遗忘。在循环神经网络忘记了部分之前的状态后，它还需要从当前的输入补充最新的记忆。这个过程就是输入门完成的。如图6所示，输入门会根据x_t\c_t-1\h_t-1决定哪些部分将进入当前时刻的状态c_t。比如当前看到文章中提到环境被污染后，模型需要将这个信息写入新的状态。通过遗忘门和输入门，LSTM结构可以更加有效的决定哪些信息应该被遗忘，哪些信息应该保留。

LSTM结构在计算得到新的状态c_t后需要产生当前时刻的输出，这个过程是通过输出门完成的，输出门会根据最新的状态c_t、上一时刻的输入h_t-1和当前的输入x_t来决定该时刻的输出h_t。比如当前的状态为被污染，那么天空的颜色后面的单词很可能就是灰色的。
