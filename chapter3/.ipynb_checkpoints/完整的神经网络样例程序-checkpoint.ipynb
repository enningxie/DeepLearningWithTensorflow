{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义神经网络的参数，输入和输出节点\n",
    "batch_size = 8\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2), name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name=\"y-input\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义前向传播过程，损失函数及反向传播算法\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.)))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 生成模拟数据集\n",
    "rdm = RandomState(1)\n",
    "X = rdm.rand(128, 2)\n",
    "Y = [[int(x1+x2 < 1)] for (x1, x2) in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1: [[-0.81131822  1.48459876  0.06532937]\n",
      " [-2.4427042   0.0992484   0.59122431]]\n",
      "w2: [[-0.81131822]\n",
      " [ 1.48459876]\n",
      " [ 0.06532937]]\n",
      "\n",
      "\n",
      "After 0 training step(s), cross entropy on all data is 0.0674925\n",
      "After 1000 training step(s), cross entropy on all data is 0.0163385\n",
      "After 2000 training step(s), cross entropy on all data is 0.00907547\n",
      "After 3000 training step(s), cross entropy on all data is 0.00714436\n",
      "After 4000 training step(s), cross entropy on all data is 0.00578471\n",
      "\n",
      "\n",
      "w1: [[-1.96182752  2.58235407  1.68203771]\n",
      " [-3.46817183  1.06982315  2.11788988]]\n",
      "w2: [[-1.82471502]\n",
      " [ 2.68546653]\n",
      " [ 1.41819501]]\n"
     ]
    }
   ],
   "source": [
    "# 创建一个会话来运行TensorFlow程序\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # 输出目前（未经训练）的参数取值\n",
    "    print(\"w1:\", sess.run(w1))\n",
    "    print(\"w2:\", sess.run(w2))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # 训练模型\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*batch_size) % 128\n",
    "        end = (i*batch_size) % 128 + batch_size\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 1000 == 0:\n",
    "            total_cross_entropy = sess.run(cross_entropy, feed_dict={x:X, y_:Y})\n",
    "            print(\"After %d training step(s), cross entropy on all data is %g\" % (i, total_cross_entropy))\n",
    "            \n",
    "    # 输出训练后的参数取值\n",
    "    print(\"\\n\")\n",
    "    print(\"w1:\", sess.run(w1))\n",
    "    print(\"w2:\", sess.run(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练上述神经网络的过程大致分为3步：  \n",
    "\n",
    "- 定义神经网络的结构和前向传播的输出结果\n",
    "- 定义损失函数以及选择反向传播优化算法\n",
    "- 生成会话（tf.Session）并且在训练数据上反复运行反向传播优化算法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
