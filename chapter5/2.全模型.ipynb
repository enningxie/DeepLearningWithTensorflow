{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设置输入和输出节点的个数，配置神经网络的参数\n",
    "INPUT_NODE = 784  # 输入节点\n",
    "OUTPUT_NODE = 10  # 输出节点\n",
    "LAYER1_NODE = 500  # 隐藏层节点数\n",
    "\n",
    "BATCH_SIZE = 100  # 每次batch打包的样本个数\n",
    "\n",
    "# 模型相关的参数\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARAZTION_RATE = 0.0001\n",
    "TRAINING_STEPS = 5000\n",
    "MOVING_AVERAGE_DECAY = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义辅助函数来计算前向传播结果，使用ReLU作为激活函数\n",
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    # 不使用滑动平均\n",
    "    if avg_class == None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "    else:\n",
    "        # 使用滑动平均\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE],name='y-input')\n",
    "    # 生成隐藏层的参数\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1,shape=[LAYER1_NODE]))\n",
    "    # 生成输出层的参数\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "    \n",
    "    # 计算不含滑动平均类的前向传播结果\n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # 定义训练轮数及相关的滑动平均类\n",
    "    # 定义存储训练轮次的变量，这个变量不需要计算滑动平均值，所以trainable=False\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    # 给定滑动平均衰减率和训练轮次变量，初始化滑动平均类，给定训练轮次的变量可以加快训练早期变量的更新速度\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    # 在所有代表神经网络参数的变量上使用滑动平均\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    # 计算使用滑动平均类的前向传播结果\n",
    "    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # 计算交叉熵及其平均值\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # 计算L2正则化损失函数\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n",
    "    regulariztion = regularizer(weights1) + regularizer(weights2)\n",
    "    loss = cross_entropy_mean + regulariztion\n",
    "    \n",
    "    # 设置指数衰减的学习率\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,\n",
    "                                              global_step,\n",
    "                                               mnist.train.num_examples / BATCH_SIZE,\n",
    "                                               LEARNING_RATE_DECAY,\n",
    "                                               staircase=True\n",
    "                                              )\n",
    "    \n",
    "    # 优化损失函数\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # 反向传播更新参数和更新每一个参数的滑动平均值\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    \n",
    "    # 计算正确率\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # 初始化会话并开始训练过程\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        validate_feed = {x:mnist.validation.images, y_: mnist.validation.labels}\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "        \n",
    "        # 循环的训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            if i % 1000 == 0:\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training steps, validation accuracy using averaeg model is %g\" % (i, validate_acc))\n",
    "            \n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x:xs, y_:ys})\n",
    "        \n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print(\"After %d training step(s), test accuracy using average model is %g\" %(TRAINING_STEPS, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "After 0 training steps, validation accuracy using averaeg model is 0.1118\n",
      "After 1000 training steps, validation accuracy using averaeg model is 0.9774\n",
      "After 2000 training steps, validation accuracy using averaeg model is 0.9818\n",
      "After 3000 training steps, validation accuracy using averaeg model is 0.9822\n",
      "After 4000 training steps, validation accuracy using averaeg model is 0.9842\n",
      "After 5000 training step(s), test accuracy using average model is 0.9816\n"
     ]
    }
   ],
   "source": [
    "# 主程序入口，这里设定模型训练的次数为5000次\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"../datasets/MNIST_data/\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练初期，随着训练的进行，模型在验证数据集上的表现越来越好，但是训练后期模型在验证集上的表现开始出现波动，这说明模型已经接近极小值了，所以迭代也就可以结束了。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
